# -*- coding: utf-8 -*-
"""DVHB.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_qp4rDEdKvUXOevakAo_tmx67vO4RYMv

# Задание и источники данных

[Задание (Notion)](https://www.notion.so/DVHB-d3be8306f85e40dd842d23056dbf5359)

[Датасет (Kaggle)](https://www.kaggle.com/denisthemartyr/dvhb-hackathon/)

[Классификация кодов культур и групп](https://geoservices.ign.fr/documentation/diffusion/documentation-offre.html)

[Список кодов](https://geoservices.ign.fr/ressources_documentaires/Espace_documentaire/BASES_VECTORIELLES/RPG/Codification_cultures_principales.csv)
"""

from google.colab import drive
drive.mount('/content/drive')

"""# Подготовка данных

**Скачиваем датасет**
"""

import os
os.chdir("/content")

"""**Скачиваем на Colab датасет archive.zip (train, test) and raw_archive.zip (2010-2014)**"""

!mkdir -p dvhb_data dvhb_data/raw_data && wget -q -O dvhb_data/archive.zip "http://35.156.82.253:8000/archive.zip" && unzip -qq dvhb_data/archive.zip -d dvhb_data && wget -q -O dvhb_data/raw_data.zip "http://35.156.82.253:8000/raw_archive.zip" && unzip -qq dvhb_data/raw_data.zip -d dvhb_data/raw_data && echo "Dataset unpack done!"

#!mkdir -p dvhb_data dvhb_data/train && wget --load-cookies /tmp/cookies.txt "https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1ONDEAzm8UmLZ-dldLSCW-rGhxWeVTm49' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\1\n/p')&id=1ONDEAzm8UmLZ-dldLSCW-rGhxWeVTm49" -O dvhb_data/train/grouped_train_full.csv && rm -rf /tmp/cookies.txt

grouped_df_train = pd.read_csv("http://35.156.82.253:8000/grouped_train_full.csv")

"""**Скачиваем на Colab наш датасет с переводом культур и групп (это модифицированный датасет [Классификация кодов культур и групп](https://geoservices.ign.fr/ressources_documentaires/Espace_documentaire/BASES_VECTORIELLES/RPG/Codification_cultures_principales.csv))**"""

!wget -q -O dvhb_data/class_cult_translate.csv "http://35.156.82.253:8000/class_cult_translate.csv"

!pip install tqdm

"""## **Единое обозначение полей для всех данных:**
---
**CODE_CULT = (код культуры например: "PPH")**

**CODE_GROUP = (код группы культуры например: "18", всего 28)**

**CENTROID = (координаты центра поля например: "Point (907753.82834152 6554634.46488151")**

**YEAR = (год из какого имени файла датасета брались данные, например: "2015")**

**LIBELLE_CULT = (наименование культуры на французском языке, например: "Fourrage")**

**LIBELLE_CULT_RUS = (наименование культуры на русском языке, например: "Фураж")**

**GROUP_CULT = (наименование группы культуры на французском языке, например: "Autres céréales")**

**GROUP_CULT_RUS = (наименование группы культуры на русском языке, например: "Другие злаки")**

**NUM_ILOT = (какой-то уникальный идентификатор, например: "006-603")**

**NOTES = (пометки)**

---

**Функция my_full_cvs сливает вместе файлы *.csv из папки train (2015-2019) в один файл train_full.csv для тренировочной выборки**
"""

import os
import glob
import re
import pandas as pd
from typing import Tuple, Optional
from tqdm import tqdm

def my_full_cvs(path, save_filename):
  dataframes = []
  os.chdir(path)
  extension = 'csv'
  all_filenames = [i for i in glob.glob('*.{}'.format(extension))]
  for f in sorted(all_filenames):
    df = pd.read_csv(f, low_memory=False)
    year = re.search(r'\d{4}', f)
    if year:
        df['year'] = year.group(0)
        dataframes.append(df)
  combined = pd.concat(dataframes, ignore_index=True)
  combined.to_csv(save_filename, index=False)
  os.chdir("/content")

my_full_cvs("dvhb_data/train", "train_full.csv")

df_train_full_new_names=['CODE_CULT', 'CODE_GROUP', 'CENTROID','YEAR']

df_train_full = pd.read_csv("dvhb_data/train/train_full.csv", names=df_train_full_new_names,skiprows=1)

df_train_full.head()

"""**Делаем единый файл test_full_2019.csv (2015-2018) из папки test/test 2019 для тестовой выборки**"""

my_full_cvs("dvhb_data/test/test 2019/", "test_full_2019.csv")

df_test_full_2019_new_names=['CODE_CULT', 'CODE_GROUP', 'CENTROID','YEAR']

df_test_full_2019 = pd.read_csv("dvhb_data/test/test 2019/test_full_2019.csv", names=df_test_full_2019_new_names,skiprows=1)

df_test_full_2019.head(1)

"""**Делаем единый файл test_full_2020.csv (2015-2019) из папки test/test 2020 для тестовой выборки**"""

my_full_cvs("dvhb_data/test/test 2020/", "test_full_2020.csv")

df_test_full_2020_new_names=['CODE_CULT', 'CODE_GROUP', 'CENTROID','YEAR']

df_test_full_2020 = pd.read_csv("dvhb_data/test/test 2020/test_full_2020.csv",names=df_test_full_2020_new_names,skiprows=1)

df_test_full_2020.head(1)

"""**Добавляем в датасет df_train_full столбцы наименование культур и групп, а так же их перевод (всего 4 столбца LIBELLE_CULT, LIBELLE_CULT_RUS, GROUP_CULT, GROUP_CULT_RUS )**"""

df_class_cult = pd.read_csv("dvhb_data/class_cult_translate.csv",sep=',')

df_class_cult = df_class_cult.drop(['CODE_GROUP'], axis=1)

df_train_full = pd.merge(df_train_full, df_class_cult, left_on='CODE_CULT', right_on='CODE_CULT')

df_train_full.head(3)

test = pd.read_csv("dvhb_data/raw_data/2010.csv")

df_train_cult_full = pd.read_csv("dvhb_data/raw_data/train_cult_full.csv", names=df_train_cult_full_new_names, skiprows=1)

test

my_full_cvs("dvhb_data/raw_data", "train_cult_full.csv")

df_train_cult_full_new_names=['CODE_GROUP', 'CENTROID','YEAR']

df_train_cult_full = pd.read_csv("dvhb_data/raw_data/train_cult_full.csv")

df_train_cult_full.head()

"""# Светлана"""

import gensim
from gensim import corpora
from pprint import pprint

# делаю маленький датасет

size = 18074
com1 = df_train_full.sample(size).copy()

# удаляю лишнее из координат
def stri(ee):
  result  = re.sub(r'[^0-9. ]+','', ee) # заменяю 
  return result

com1['centroid_n'] = com1['CENTROID'].apply(stri)

# кодирую слова векторами
text = [com1['CODE_CULT'].tolist()]
dictionary = corpora.Dictionary(text)

#заменяем значения в столбце object_name_n на данные из словаря, а ключи берем из столбца object_type_number
com1['CODE_CULTU_COD'] = com1['CODE_CULT'].map(dictionary.token2id)

# режем по пробелу
street = com1['centroid_n'].str.split(' ',expand=True)
com1 = com1.join(street)

com1 = com1[['CODE_CULTU_COD','CODE_GROUP','YEAR',1,2]]

import matplotlib.pyplot as plt 
from sklearn.cluster import KMeans
from scipy.cluster.hierarchy import dendrogram, linkage 
import seaborn as sns
y = com1['CODE_CULTU_COD']
X = com1.drop(['CODE_CULTU_COD'], axis=1)

# долго считает дендрограмму
#linked = linkage(X, method = 'ward') 
#plt.figure(figsize=(15, 10))  
#dendrogram(linked, orientation='top')
#plt.show()

# думала или на 22 кластера или на  6 -надо посмтреть качество разбиения
n_clusters = 22
km = KMeans(n_clusters = n_clusters) # задаём число кластеров, равное 22
labels1 = km.fit_predict(X[[1,2]]) # применяем алгоритм к данным и формируем вектор кластеров 

n_clusters = 6
km = KMeans(n_clusters = n_clusters) # задаём число кластеров, равное 6
labels2 = km.fit_predict(X[[1,2]])

com1['cluster_km1'] = labels1
com1['cluster_km2'] = labels2

com1.head(5)

# возьмем данные для более общих областей
data = com1.pivot_table(index = ['cluster_km2','CODE_GROUP'], columns = ['YEAR'], values = 'CODE_CULTU_COD', aggfunc = 'mean')

fig, ax = plt.subplots(figsize=(8,8))
sns_heatmap = sns.heatmap(data.isnull(), yticklabels=False, cbar=False, cmap='viridis')

# возьмем данные для  мелких областей
data = com1.pivot_table(index = ['cluster_km1','CODE_CULTU_COD'], columns = ['YEAR'], values = 'CODE_GROUP', aggfunc = 'mean').\
        sort_values(by = [2015,2016,2017,2018,2019])

fig, ax = plt.subplots(figsize=(12,20))
sns_heatmap = sns.heatmap(data.isnull(), yticklabels=False, cbar=False, cmap='viridis')

"""#Павел"""

df_train_full[df_train_full['CENTROID'] == 'Point (1000001.31785654 6472025.93934252)']

def get_coords(coords: str) -> Tuple[Optional[float], Optional[float]]:
    '''
    Разбирает строковое представление координат

    :param coords: сроковое представление координат
    :return tuple кортеж из полученных координат
    '''
    regex = re.search('(?P<lang>\d+\.?\d*)\s(?P<long>\d+\.?\d*)', coords)
    return regex.group('lang'), regex.group('long')

df_train_grouped_full = df_train_full.groupby('CENTROID', dropna=False).agg(list)

df_train_grouped_full.head()

"""ппц сколько считаеться - у меня 1 % посчитался за 2 минуты"""

df_train_grouped_full

columns = [f'{column}_{year}' for column in df_train_full.columns.drop(['CENTROID', 'YEAR']) for year in df_train_full['YEAR'].unique()] + ['LATITUDE', 'LONGTITUDE']
new_df_train = pd.DataFrame(columns=columns)
new_df_train.to_csv("grouped_train_full.csv", index=False)

for row in tqdm(df_train_grouped_full.iterrows(), total=len(df_train_grouped_full)):
    if len(new_df_train) == 1000:
        new_df_train.to_csv("grouped_train_full.csv", mode='a', header=False, index=False, columns=columns)
        del new_df_train
        new_df_train = pd.DataFrame(columns=columns)
    regex = re.search('(?P<lat>\d+\.?\d*)\s(?P<long>\d+\.?\d*)', row[0])
    new_df_train = new_df_train.append({**{f'{column}_{year}': row[1][column][i] for column in row[1].keys().drop('YEAR') for i, year in enumerate(row[1]['YEAR'])}, **{'LATITUDE': regex.group('lat'), 'LONGTITUDE': regex.group('long')}}, ignore_index=True)
    

new_df_train.head()

grouped_df_train = pd.read_csv("dvhb_data/train/grouped_train_full.csv")

grouped_df_train

com1 = grouped_df_train.sample(361400).copy()
pd.save_csv("com1.csv")

grouped_df_train.isnull().sum()

grouped_df_train.sample(n = int(len(grouped_df_train)/10)).to_csv('sample_10.csv')

grouped_df_train = pd.read_csv("http://35.156.82.253:8000/grouped_train_full.csv")

grouped_df_train.head(1)

import gensim
from gensim import corpora
from pprint import pprint
from sklearn.model_selection import train_test_split
!pip install catboost
from catboost import CatBoostClassifier
from sklearn.metrics import confusion_matrix, classification_report,roc_auc_score
from sklearn.metrics import accuracy_score

from catboost.utils import eval_metric
# Пашин датасет, оставлю только растения и место
df_test_swetlana = grouped_df_train[['CODE_CULT_2015','CODE_CULT_2016','CODE_CULT_2017','CODE_CULT_2018','CODE_CULT_2019','LATITUDE','LONGTITUDE']].copy()

# кодирую слова векторами
text = [df_train_full['CODE_CULT'].tolist()]
dictionary = corpora.Dictionary(text)

#заменяем значения в столбце object_name_n на данные из словаря, а ключи берем из столбца object_type_number
df_test_swetlana['CODE_CULT_2019'] = df_test_swetlana['CODE_CULT_2019'].map(dictionary.token2id)
df_test_swetlana['CODE_CULT_2018'] = df_test_swetlana['CODE_CULT_2018'].map(dictionary.token2id)
df_test_swetlana['CODE_CULT_2017'] = df_test_swetlana['CODE_CULT_2017'].map(dictionary.token2id)
df_test_swetlana['CODE_CULT_2016'] = df_test_swetlana['CODE_CULT_2016'].map(dictionary.token2id)
df_test_swetlana['CODE_CULT_2015'] = df_test_swetlana['CODE_CULT_2015'].map(dictionary.token2id)

# делим
X = df_test_swetlana.drop(['CODE_CULT_2019', ], axis=1)
y = df_test_swetlana['CODE_CULT_2019']

features_train, features_valid, target_train, target_valid = train_test_split(
    X, y, test_size=0.25, random_state=12345)


model = CatBoostClassifier(verbose=100,
                           learning_rate=0.7,
                           early_stopping_rounds=200,
                           eval_metric='AUC',
                           #task_type="GPU",
                           )

model.fit(features_train, target_train) 
predictions_valid = model.predict(features_valid) 

print(accuracy_score(target_valid, predictions_valid))

